{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read the dataset into pandas dataframe\n",
    "df = pd.read_csv('./../datasets/downsampled_data', delim_whitespace=False).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first five rows\n",
    "retrieval_time = df['time_retrieved']\n",
    "publish_time = df['publishedAt']\n",
    "channel_publish_time = df['Channel_publishedAt']\n",
    "retrieval_time_11_19_14 = df['11_19_14_update_timestamp']\n",
    "columns_to_drop = ['definition', 'publishedAt', 'time_retrieved', 'Channel_title', '11_19_14_update_timestamp', 'Channel_publishedAt', 'video_id', 'channelId', 'thumbnail_link', 'Channel_country']\n",
    "df = df.drop(columns_to_drop, axis = 1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## time updates\n",
    "import dateutil.parser as parser\n",
    "age = []\n",
    "age_update = []\n",
    "channel_age = []\n",
    "for i in df.index:\n",
    "    channel_publish_time[i] = channel_publish_time[i].replace(\"\\\"\", \"\")\n",
    "    age.append(parser.isoparse(retrieval_time[i]) - parser.isoparse(publish_time[i]))\n",
    "    age_update.append(parser.isoparse(retrieval_time_11_19_14[i]) - parser.isoparse(publish_time[i]))\n",
    "    channel_age.append(parser.isoparse(channel_publish_time[i]) - parser.isoparse(publish_time[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sentiment values\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "titles = df['title']\n",
    "channel_title = df['channelTitle']\n",
    "description = df['description']\n",
    "channel_description = df['Channel_description']\n",
    "\n",
    "title_sentiment_vals = []\n",
    "channel_title_sentiment_vals = []\n",
    "description_sentiment_vals = []\n",
    "channel_description_sentiment_vals = []\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sentence in titles:\n",
    "    ss = sid.polarity_scores(str(sentence))\n",
    "    title_sentiment_vals.append(ss['pos']-ss['neg'])\n",
    "    \n",
    "for sentence in channel_title:\n",
    "    ss = sid.polarity_scores(str(sentence))\n",
    "    channel_title_sentiment_vals.append(ss['pos']-ss['neg'])\n",
    "    \n",
    "for sentence in description:\n",
    "    ss = sid.polarity_scores(str(sentence))\n",
    "    description_sentiment_vals.append(ss['pos']-ss['neg'])\n",
    "    \n",
    "for sentence in channel_description:\n",
    "    ss = sid.polarity_scores(str(sentence))\n",
    "    channel_description_sentiment_vals.append(ss['pos']-ss['neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df['title'] = title_sentiment_vals\n",
    "df['channelTitle'] = channel_title_sentiment_vals\n",
    "df['description'] = description_sentiment_vals\n",
    "df['Channel_description'] = channel_description_sentiment_vals\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "df['trending?'] = le.fit_transform(df['trending?'])\n",
    "df['ratings_disabled'] = le.fit_transform(df['ratings_disabled'])\n",
    "df['Channel_hiddenSubscriberCount'] = le.fit_transform(df['Channel_hiddenSubscriberCount'])\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy values for now - get these interactively later...\n",
    "import tensorflow as tf\n",
    "\n",
    "best_model = True\n",
    "if best_model:\n",
    "    num_hidden_layers = 3\n",
    "    num_hidden_layer_nodes = [20, 10, 5]\n",
    "    train_ratio = .7\n",
    "    hidden_layer_activations = ['sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid']\n",
    "    optimizer = 'sgd'\n",
    "    learning_rate = .005\n",
    "    loss = 'mean_squared_error'\n",
    "    metrics = [tf.keras.metrics.Accuracy(),tf.keras.metrics.Recall(),tf.keras.metrics.Precision()]\n",
    "    metrics_names = [\"accuracy\",\"recall\",\"precision\"]\n",
    "    epochs = 300\n",
    "    batch_size = 200\n",
    "else:\n",
    "    # build a custom model\n",
    "    num_hidden_layers = 3\n",
    "    num_hidden_layer_nodes = [20, 10, 5]\n",
    "    train_ratio = .7\n",
    "    hidden_layer_activations = ['sigmoid', 'sigmoid', 'sigmoid', 'sigmoid', 'sigmoid']\n",
    "    optimizer = 'sgd'\n",
    "    learning_rate = .005\n",
    "    loss = 'mean_squared_error'\n",
    "    metrics = [\"accuracy\"]\n",
    "    metrics_names = [\"accuracy\"]\n",
    "    epochs = 300\n",
    "    batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set by 70:30\n",
    "ratio = 0.7\n",
    "train, test = train_test_split(df, train_size=ratio, random_state=42)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data into x and y - just random y for now..\n",
    "train_X = train.loc[:,train.columns != 'trending?']\n",
    "train_Y = train['trending?']\n",
    "test_X = test.loc[:,test.columns != 'trending?']\n",
    "test_Y = test['trending?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the logistic regression model - need clean data...\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR_model = LogisticRegression(multi_class='ovr')\n",
    "LR_model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred_Y = LR_model.predict(test_X);\n",
    "confusion_matrix(test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the accuracy of the LR model\n",
    "accuracy = LR_model.score(test_X, test_Y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the ANN model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "ANN_model = keras.Sequential()\n",
    "\n",
    "# add input layerr\n",
    "ANN_model.add(Input(shape=train_X.shape[1]))\n",
    "\n",
    "# add hidden layers\n",
    "for i in range(num_hidden_layers):\n",
    "    ANN_model.add(Dense(num_hidden_layer_nodes[i], activation=hidden_layer_activations[i + 1]))\n",
    "\n",
    "# add output layers\n",
    "ANN_model.add(Dense(1, activation=hidden_layer_activations[len(hidden_layer_activations) - 1]))\n",
    "\n",
    "ANN_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "ANN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "ANN_model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "pred_Y = ANN_model.predict_classes(test_X);\n",
    "confusion_matrix(test_Y, pred_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report evaluation metrics \n",
    "evaluated_metrics = ANN_model.evaluate(test_X, test_Y)\n",
    "for i in range(len(metrics)):\n",
    "    print(metrics_names[i] + \": %.2f\" % evaluated_metrics[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
